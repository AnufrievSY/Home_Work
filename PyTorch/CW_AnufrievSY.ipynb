{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f982b3f5-1e59-47f0-a3cd-27d779633b95",
   "metadata": {},
   "source": [
    "# Импорт необходимых бибилиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8424575-be83-4914-b410-bd0a73a83189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'OpenCV'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Balaje/OpenCV.git\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from random import randint as rndm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86008073-0712-4ca4-a2f3-b993461f2bf4",
   "metadata": {},
   "source": [
    "# Настройка модели для распознавания жестов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8362e1-4acb-4044-af6b-9b22e33e1866",
   "metadata": {},
   "source": [
    "## Настраиваем трансформеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19c2dc97-37e9-41ac-89ac-dd797e7edc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем переменную, в которой хранятся названия папок, который и будут в дальнейшем нашими лэйблами\n",
    "labels_original = ['01_palm', '02_l', '03_fist', '04_fist_moved', '05_thumb', '06_index', '07_ok', '08_palm_moved', '09_c', '10_down']\n",
    "\n",
    "# Создаем транформер для обычной обработки изображения\n",
    "transform_default = transforms.Compose([transforms.Pad(padding=(0, 200, 0, 200),    # добавление полей сверху и снизу\n",
    "                                                       fill=(0),                    # черным цветов\n",
    "                                                       padding_mode='constant'),    # заполнение цветом переданным в fill\n",
    "                                        transforms.CenterCrop(400),                 # обрезка по центру до квадрата 400 на 400\n",
    "                                        transforms.Resize((256,256)),               # уменьшаем размер изображений\n",
    "                                        transforms.Grayscale(),                     # переводим в оттенки серого\n",
    "                                        transforms.ToTensor()])                     # преобразуем в тензор\n",
    "# Создаем трансформер для увеличения изначального объема изображений\n",
    "transform_augmentation = transforms.Compose([transform_default,                                   # за основну берем дефолтный трансформ\n",
    "                                             transforms.RandomRotation(degrees = rndm(5, 30)),    # случайный поворот от 5 до 30 градусов\n",
    "                                             transforms.RandomHorizontalFlip(),                   # случайное отражение по горизонтали\n",
    "                                             transforms.ColorJitter(                              # добавление случайных изменений\n",
    "                                                brightness=rndm(0, 9)/10,                         # яркости\n",
    "                                                contrast=rndm(0, 9)/10)                           # контраста\n",
    "                                             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed0cb11-dbc4-4e25-b6b5-87926b88f46d",
   "metadata": {},
   "source": [
    "## Подготавливаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62668c3-fbea-4011-bc89-6e21ed45dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем список в котором содержаться датасеты с изображениями из всех папок, пропущенных через дефолтный трансформер\n",
    "datasets = [torchvision.datasets.ImageFolder(root='leapGestRecog/'+people, transform=transform_default) for people in tqdm(os.listdir('leapGestRecog'))]\n",
    "# Добавляем теже изображения, но немного измененные при помощи второго трансформера\n",
    "datasets.extend([torchvision.datasets.ImageFolder(root='leapGestRecog/'+people, transform=transform_augmentation) for people in tqdm(os.listdir('leapGestRecog'))])\n",
    "# Объединяем это все в единый датасет\n",
    "dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "# Разбиваем на тренировочну, валидационную и тестовую выборки\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "     [int(len(dataset) * 0.7),    # Тренировочная выборка\n",
    "      int(len(dataset) * 0.2),    # Валидационная выборка\n",
    "      int(len(dataset) * 0.1)],   # Тестовая выборка\n",
    "  generator=torch.Generator().manual_seed(42))\n",
    "# Задаем загрузчики данных\n",
    "get_loader = lambda data: DataLoader(data, batch_size=16, shuffle=True, num_workers = 2)\n",
    "batch_size = 16\n",
    "train_loader, val_loader, test_loader = get_loader(train_dataset), get_loader(val_dataset), get_loader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d94427-7de6-4099-9ccb-32415cff7578",
   "metadata": {},
   "source": [
    "## Прописываем архитектуру модели\n",
    "В данном случае, в процессе поиска различных вариантов, было выявленно, что эти две модели справляются довольно хорошо с поставленной задачей. В качестве эксперимента было принято решение объединить их. В дальнейшем ни одна другая модель не справлялась лучше, чем этот вариант. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbf7df32-5332-4663-ae23-f80e551cf894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 61 * 61, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 61 * 61)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class New_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(New_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 61 * 61, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.bn1(nn.functional.leaky_relu(self.conv1(x))))\n",
    "        x = self.pool(self.bn2(nn.functional.leaky_relu(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 61 * 61)\n",
    "        x = nn.functional.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.functional.leaky_relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        for param in self.model1.parameters():\n",
    "            param.requires_grad = False\n",
    "        out_features = model1.fc3.out_features\n",
    "        self.fc3 = nn.Linear(2*out_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.model1(x)\n",
    "        out2 = self.model2(x)\n",
    "        out = torch.cat((out1, out2), 1)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bab417-4374-4177-aac7-286f85632bcc",
   "metadata": {},
   "source": [
    "## Собственная функция точности модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d231d1-052f-4c65-a24d-4d331017dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получает модель для тестирования, возвращает среднее значения удаленности истинного значения от предсказанного\n",
    "def check(m):\n",
    "  r = []\n",
    "\n",
    "  # Оценка насколько правильное значения близко к предсказанному\n",
    "  # Идеально когда 0\n",
    "  my_accuracy = lambda pred, true: pred.max() - pred[0][true]\n",
    "\n",
    "  for name in os.listdir('my_test_image/'):\n",
    "\n",
    "    # Загружаем изображение\n",
    "    test_image_path = f'my_test_image/{name}'\n",
    "    # Загружаем руку модулем подходящим для модели по поиску координат руки на изображении\n",
    "    img = cv2.imread(test_image_path)\n",
    "    # Инициализация MediaPipe Hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands()\n",
    "    # Обнаружение ключевых точек на руке на изображении, которое мы трансформировали в RGB формат из дефолтного для CV2 BGR формата\n",
    "    results = hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    # Прописываем координнаты точек по осям X и Y в свои списки\n",
    "    x_list = [p.x for p in results.multi_hand_landmarks[0].landmark]\n",
    "    y_list = [p.y for p in results.multi_hand_landmarks[0].landmark]\n",
    "    # Определяем координаты квадрата, в котором расположена рука\n",
    "    x = int(min(x_list)*img.shape[1])-20\n",
    "    y = int(min(y_list)*img.shape[0])-20\n",
    "    w = int(max(x_list)*img.shape[1])-int(min(x_list)*img.shape[1])+40\n",
    "    h = int(max(y_list)*img.shape[0])-int(min(y_list)*img.shape[0])+40\n",
    "\n",
    "    # Загружаем руку модулем, удобным для нас\n",
    "    test_image = Image.open(test_image_path)\n",
    "    # Обрезаем изображения так, чтобы осталась одна только рука\n",
    "    test_image_hand = test_image.crop((x-20, y-20, x+w+40, y+h+40))\n",
    "    # Подготовка изображения для модели\n",
    "    image_tensor = transform_default(test_image_hand).unsqueeze(0)\n",
    "    # Получаем предсказания\n",
    "    pred = m(image_tensor)\n",
    "    # Добавляем их в список результатов\n",
    "    r.append(float(my_accuracy(pred, [l[3:] for l in labels_original].index(name.split('.')[0]))))\n",
    "    pred = pred.argmax()\n",
    "  return sum(r)/len(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202fa3d3-bfdb-43e3-b620-406e600347b0",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9beff-dc18-40c2-b6f6-f11876e91475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение устройства вычислений\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Инициализация модели и оптимизатора\n",
    "model = CombinedModel(Model().to(device), New_Model().to(device))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.2) # по результатам тестов лучшее сочетание\n",
    "\n",
    "# Задаем количество эпох для обучения модели\n",
    "num_epochs = 20\n",
    "# Запускаем цикл обучения\n",
    "for epoch in range(num_epochs):\n",
    "  # Инициализация переменной потерь для текущей эпохи\n",
    "  running_loss = 0.0\n",
    "  # Инициализация списков для хранения значений потерь и метрик для последующей оценки модели\n",
    "  loss_list, acc_list = []\n",
    "  # Переводим модель в режим обучения\n",
    "  model.train()\n",
    "  # Создаем индикатор прогресса обучения с общим числом итераций равным размеру обучающего набора данных\n",
    "  with tqdm(total=len(train_loader)) as pbar:\n",
    "      # Цикл по данным из обучающего набора\n",
    "      for data in train_loader:\n",
    "          # Обнуление градиентов оптимизатора\n",
    "          optimizer.zero_grad()\n",
    "          # Получаем отдельно изображение и его лэйбл\n",
    "          inputs, labels = data[0].to(device), data[1].to(device)\n",
    "          # Прямой проход через модель\n",
    "          outputs = model(inputs)\n",
    "          # Вычисление функции потерь\n",
    "          loss = criterion(outputs, labels)\n",
    "          # Вычисление градиентов\n",
    "          loss.backward()\n",
    "          # Обновление весов модели\n",
    "          optimizer.step()\n",
    "          # Добавление текущей потери к общей потере\n",
    "          running_loss += loss.item()\n",
    "          # Вычисление точности для текущего пакета данных\n",
    "          acc = 100 * sum((np.array([int(o.argmax()) for o in outputs]) == np.array(labels)).astype(int)) / len(outputs)\n",
    "          # Обновление прогресс бара информацией о том сколько эпох пройдено, потерях и точности\n",
    "          pbar.set_description(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "          pbar.postfix = (f'| Loss: {loss:.3f} | Accuracy: {acc:.2f}%')\n",
    "          pbar.update()\n",
    "  # Вывод значения кросс-энтропии для текущей эпохи\n",
    "  print(f'\\nCrossEntropyLoss (train): {running_loss / len(train_loader):.5f}')\n",
    "\n",
    "  # Переводим модель в режим валидации\n",
    "  model.eval()\n",
    "  # Инициализация переменной потерь для валидации\n",
    "  running_loss = 0.0\n",
    "  # Инициализация переменных для подсчета точности модели на тестовой выборке\n",
    "  correct, total = 0, 0\n",
    "  # Выключаем отслеживание градиентов для валидации\n",
    "  with torch.no_grad():\n",
    "      # Цикл по данным из тестового набора\n",
    "      for data in tqdm(test_loader):\n",
    "          # Распределение входных данных и меток по устройству (обычно GPU)\n",
    "          inputs, labels = data[0].to(device), data[1].to(device)\n",
    "          # Прямой проход через модель\n",
    "          outputs = model(inputs)\n",
    "          # Получение метки с максимальным значением выхода\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          # Инкрементирование общего количества\n",
    "          total += labels.size(0)\n",
    "          # Инкрементирование количества правильно классифицированных объектов\n",
    "          correct += (predicted == labels).sum().item()\n",
    "  # Вывод точности на данных из набора\n",
    "  print(f'Accuracy (val): %d %% \\n' % (100 * correct / total))\n",
    "  # Вывод точности на данных из собственного набора\n",
    "  print(f'Accuracy (my_test): {check(model)}')\n",
    "\n",
    "  # Сохранение текущей модели на диск\n",
    "  torch.save({'transformer': transform_default,\n",
    "              'model_state_dict': model.state_dict()},\n",
    "            f'Models/model_e{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d5c31-a066-4f8c-a4e4-9b5723c7ffa6",
   "metadata": {},
   "source": [
    "### Результаты обучения самых различных вариантов (не только этого)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a859a-93d2-4fa5-8a78-b2cc2b43e15e",
   "metadata": {},
   "source": [
    "**Version 1**  \n",
    "* image_size = (16, 16)  \n",
    "* batch_size = 4  \n",
    "* num_epoch = 10  \n",
    "* Обучена обсолютно на всех данных что есть. \n",
    "\n",
    "Точность - 99.99%  \n",
    "Скорость обучения - 21 минут 45 секунды\n",
    "\n",
    "--- \n",
    "\n",
    "**Version 2**  \n",
    "* image_size = (16, 16)  \n",
    "* batch_size = 16  \n",
    "* num_epoch = 10  \n",
    "* Изначальные данные были разделены на тренировочную и тестовую выборки (соотношение 80/20).\n",
    "\n",
    "Точность - 99.85%  \n",
    "Функция потерь - 0.01648  \n",
    "Скорость обучения - 13 минут 22 секунд\n",
    "\n",
    "---\n",
    "\n",
    "**Version 2.1**\n",
    "* Изменения трансформера, модель теперь принимает чернобелые изображения размеров 250*250  \n",
    "\n",
    "Epoch 1/10:   \n",
    "* CrossEntropyLoss (train): 2.30379  \n",
    "* Accuracy (val): 9 % \n",
    "\n",
    "Epoch 10/10:  \n",
    "* CrossEntropyLoss (train): 2.28108  \n",
    "* Accuracy (val): 15 %  \n",
    "\n",
    "Точность модели (test): 9 %\n",
    "\n",
    "---\n",
    "\n",
    "**Version 2.2**  \n",
    "* из агуметации убраны: поврот по вертикали, увеличения красочности, насыщенности  \n",
    "* в аугментацию добавлены: доведение до квадрата путем заполнения пустот черным цветом (чтобы не размазывало изображения), обрезка изображения по центру рандомизация поворот, яркости и контрастности.\n",
    "* обучена на лучшей первой версии модели  \n",
    "\n",
    "Epoch 1/20: \n",
    "* CrossEntropyLoss (train): 2.27711\n",
    "* Accuracy (val): 19 % \n",
    "\n",
    "Epoch 10/20:\n",
    "* CrossEntropyLoss (train): 1.37242\n",
    "* Accuracy (val): 61 %\n",
    "\n",
    "Epoch 20/20:\n",
    "* CrossEntropyLoss (train): 0.44998\n",
    "* Accuracy (val): 88 %\n",
    "\n",
    "Точность модели (test): 10 %\n",
    "\n",
    "---\n",
    "\n",
    "**Version 3 (final)**\n",
    "* Выбрана лучшая из всех модель для дообучения\n",
    "* Написана еще одна модель и объединена с лучшей  \n",
    "* Была написанна функция, которая возвращает среднюю дальность предсказанного класса от истинного класса\n",
    "\n",
    "Epoch 1/1: \n",
    "* CrossEntropyLoss (train): 2.01468\n",
    "* Accuracy (val): 62 %\n",
    "* Accuracy (test): 9 %\n",
    "\n",
    "Epoch 2/20:\n",
    "* CrossEntropyLoss (train): 1.99220\n",
    "* Accuracy (val): 62 %\n",
    "* Accuracy (my_test): 0,6804 \n",
    "\n",
    "Epoch 3/20: \n",
    "* CrossEntropyLoss (train): 1.13265\n",
    "* Accuracy (val): 83 %\n",
    "* Accuracy (my_test): 0,9211\n",
    "\n",
    "Epoch 4/20: \n",
    "* CrossEntropyLoss (train): 0.88938\n",
    "* Accuracy (val): 87 %\n",
    "* Accuracy (my_test): 1.01267\n",
    "\n",
    "Epoch 5/20:\n",
    "* CrossEntropyLoss (train): 0.65368\n",
    "* Accuracy (val): 93 %\n",
    "* Accuracy (my_test): 1.2956\n",
    "\n",
    "Epoch 6/20: \n",
    "* CrossEntropyLoss (train): 0.50916\n",
    "* Accuracy (val): 95 %\n",
    "* Accuracy (my_test): 1.36023\n",
    "\n",
    "--- \n",
    "\n",
    "**Вывод**\n",
    "По результам было принято решение не обучать модель до конца, а остановиться на второй эпохе, т.к. в дальнейшем она начинала переобучаться и на реальных данных всем изображениям прописывала один, максимум два типа."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c58ede-a46c-44f4-955c-faec1576fac6",
   "metadata": {},
   "source": [
    "## Загружаем модель, которая показала лучшие результаты\n",
    "Модель которая дошла до 3 эпоих показал себя лучше всего, было принято решение взять ее за основу.  \n",
    "palm - распознает примерно в 50%  \n",
    "ok - распознает в 100%  \n",
    "down - если ближе к камере, но приммерно в 25%  \n",
    "index - если повернуть руку примерно на 45 градусов, но примерно в 25%  \n",
    "все жесты в целом лучше распознаются если показывать их правой рукой  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d6b9bea-0506-4c86-ae7e-1019cb86f928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CombinedModel(Model(), New_Model())\n",
    "model.load_state_dict(state_dict = torch.load('model_e2.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a186dcb-d69f-4c2e-851b-34b59acfe3d8",
   "metadata": {},
   "source": [
    "# Настройка модели для поиска лица на изображении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb69bfe2-ec61-4f74-b9d7-8576203fdeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_face(img):\n",
    "    # преобразование изображения в оттенки серого\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # создание экземпляра класса CascadeClassifier для поиска объектов\n",
    "    haar_cascade = cv2.CascadeClassifier('C:\\\\PyProject\\\\Home_Work\\\\PyTorch\\\\Course_Work\\\\OpenCV\\\\haarcascades\\\\face.xml')\n",
    "    # поиск лица на изображении\n",
    "    face = haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    # обводим найденное лицо\n",
    "    for (x, y, w, h) in face:\n",
    "      cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11313a3e-93af-47ed-be7e-5a85c2b4a832",
   "metadata": {},
   "source": [
    "# Основной код\n",
    "Захват изображения с камеры, обработка всеми ранее прописанными функциями и моделями. Вывод на экране пользователю изображения с камеры, с найденным лицом, рукой и предсказанием типа жеста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5dc12d7d-accc-45a9-ab6b-c2fed7faae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прописываем названия классов\n",
    "labels = ['palm', 'l', 'fist', 'fist_moved', 'thumb', 'index', 'ok', 'palm_moved', 'c', 'down']\n",
    "# Создаем датафрем для записис результатов\n",
    "df = pd.DataFrame([[[], 0, 0] for _ in range(len(labels))], columns=['values', 'frequency', 'probability'], index=labels)\n",
    "\n",
    "# Создаем объекты для обнаружения рук и рисования ключевых точек\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Создаем объект VideoCapture для захвата видео с веб-камеры\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Проверяем успешность открытия видео устройства\n",
    "if not cap.isOpened():\n",
    "    print(\"Не удалось открыть видео устройство\")\n",
    "    exit()\n",
    "\n",
    "# Запускаем бесконечный цикл для захвата и обработки изображения с камеры\n",
    "while True:\n",
    "    # Захватываем кадр из видео\n",
    "    ret, frame = cap.read()\n",
    "    # Проверяем успешность захвата кадра\n",
    "    if not ret:\n",
    "        print(\"Не удалось получить кадр\")\n",
    "        break\n",
    "    frame = find_face(frame)\n",
    "    # Преобразуем кадр из BGR в RGB и пытаемся обнаружить руки в кадре\n",
    "    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cat_frame = np.array([[], []])\n",
    "    # Начинаем работу с изображением если рука найдена, если нет, то происходит отчистка датафрема с результатами\n",
    "    if results.multi_hand_landmarks:\n",
    "        # отмечаем ключевые точки руки\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        # обрезка изображения по наивысшим точкам каждой оси с допуском в 40 пикселей\n",
    "        x_list = [p.x for p in results.multi_hand_landmarks[0].landmark]\n",
    "        y_list = [p.y for p in results.multi_hand_landmarks[0].landmark]\n",
    "        h, w, c = frame.shape\n",
    "        cat_frame = frame[int(min(y_list) * h) - 40:int(max(y_list) * h) + 40, int(min(x_list) * w) - 40:int(max(x_list) * w) + 40]\n",
    "    else:\n",
    "        df = pd.DataFrame([[[], 0, 0] for _ in range(len(labels))], columns=['values', 'frequency', 'probability'], index=labels)\n",
    "\n",
    "    # если в каком-то из измерений массива изображения отсутсвуют значения, то происходит отчистка датафрема с результатами\n",
    "    if 0 not in cat_frame.shape:\n",
    "        # трансформируем изображение под модель\n",
    "        input = transform_default(F.to_pil_image(cat_frame)).unsqueeze(0)\n",
    "        # получаем предсказание от модели\n",
    "        preds = model(input).detach().numpy()[0]\n",
    "        \n",
    "        # записываем список предсказний\n",
    "        df.iloc[preds.argmax(), 0].append(preds.max())\n",
    "        # записываем частоты выбора позиции\n",
    "        df.iloc[preds.argmax(), 1] = len(df.iloc[preds.argmax(), 0])\n",
    "        # записываем вероятность того что это именно это значение\n",
    "        df.iloc[preds.argmax(), 2] = len(df.iloc[preds.argmax(), 0])/sum(df['frequency'])*100\n",
    "        # получаем значения которое выбиралось чаще всего\n",
    "        predict_label = df.loc[df['frequency'] == df['frequency'].max()]\n",
    "        #  получаем значения с наибольшей вероятностью\n",
    "        predict = predict_label.loc[predict_label['probability'] == predict_label['probability'].max()]\n",
    "        # прописываем название лэйбла, есть частоту выбора и вероятность что это именно оно\n",
    "        predict_text = f\"{predict.index[0]}: {predict['frequency'].values[0]} - {predict['probability'].values[0]:.2f}%\"\n",
    "\n",
    "        # Создание фона для текста\n",
    "        text_background = np.zeros((100, 500, 3), dtype=np.uint8)\n",
    "        # Добавление predict_text\n",
    "        cv2.putText(text_background, predict_text, (50, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        # Отображение окна\n",
    "        cv2.imshow('Predict', text_background)\n",
    "    else:\n",
    "        df = pd.DataFrame([[[], 0, 0] for _ in range(len(labels))], columns=['values', 'frequency', 'probability'], index=labels)\n",
    "\n",
    "    # Отображаем кадр с нарисованными точками в окне\n",
    "    cv2.imshow('Webcam', cv2.flip(frame, 1))\n",
    "\n",
    "    # Обработка нажатия клавиши 'q' для выхода из цикла\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Освобождаем ресурсы\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
